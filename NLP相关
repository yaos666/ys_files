**** 如何理解自然语言处理的基本原理？


分词：将输入的自然语言文本分成一个个离散的语言单元，通常是词或符号。
词性标注：对分词后的语言单元赋予相应的语法和语义标签，例如名词、动词、形容词、代词等，以便进一步分析。
句法分析：将分词和词性标注后的语言单元组织成一个句子结构，分析单词之间的语法关系，如主谓宾、定状补等。
命名实体识别：识别文本中的特定实体，如人名、地名、时间、组织机构等。
情感分析：对文本进行分类，判断文本的情感偏向，如正面、负面或中性。
机器翻译：将一种语言翻译成另一种语言，通常基于统计或神经网络模型。


一、语言的表示与处理
1、分词（Tokenization）：
	分词是将一段文本拆分成若干个有意义的单元（通常是词或字符）。例如，在中文中，分词是将连续的字符序列划分为词语，而在英文中，通常是通过空格和标点符号来进行分词。
2、词性标注（Part-of-Speech Tagging，POS Tagging）：
	词性标注是将每个单词标注为其对应的词性（如名词、动词、形容词等），帮助理解单词在句子中的作用。
3、命名实体识别（Named Entity Recognition，NER）：
	识别文本中的实体，如人名、地名、组织名等，通常用于信息提取任务。
4、句法分析（Syntactic Parsing）：
	通过构建句法树，分析句子中词语之间的结构关系，以理解语法结构。

二、语义理解
1、词向量（Word Embeddings）：
	将单词表示为高维向量，能够捕捉词语的语义信息。常见的词向量模型包括Word2Vec、GloVe、FastText等。这些词向量模型通过分析词语在大量语料中的上下文关系来学习词语的语义。
2、句子或文档表示：
	为了进一步理解和处理文本，通常需要将句子或文档转换为向量表示。常用的技术包括TF-IDF、BERT等预训练语言模型。
3、语义角色标注（Semantic Role Labeling，SRL）：
	分析句子中各个成分在语义上的角色（例如，谁是动作的执行者、谁是动作的承受者等）。



一、语言模型-概率模型
语言模型是理解和生成语言的基础。
计算一个句子在语言中出现的概率。
这一概率是通过分析大量文本数据，统计不同词汇之间的关系而得到的。
语言模型的基本原理是基于马尔可夫假设，即一个词的出现只依赖于前面几个词。

二、分词技术
将一段连续的文本分割成有意义的词语
中文分词相对于英文更为复杂，因为中文中没有像英文那样的明显的词语分隔符。
基于规则的分词和基于统计的分词是两种常见的分词技术。基于规则的分词使用人工定义的规则，
而基于统计的分词则通过分析大量文本数据学习词语的概率分布。

三、词嵌入
词嵌入是自然语言处理中的一种表示方法，它通过将词语映射到一个高维向量空间中，
使得具有相似语义的词在向量空间中距离较近。这种表示方法有助于捕捉词语之间的语义关系，
从而提高模型在理解语言任务上的性能。Word2Vec、GloVe和BERT等模型是常用的词嵌入模型。
bce-embedding-base_v1、bge-large-zh-v1.5、bge-m3

四、语法分析
语法分析是自然语言处理中的一个关键任务，其目标是分析句子的结构和语法关系。
通过语法分析，计算机可以理解句子中各个词语之间的依赖关系，从而更好地理解句子的含义。
常见的语法分析方法包括基于规则的语法分析和基于统计的语法分析。

五、信息抽取
信息抽取是自然语言处理中的一项重要任务，其目标是从大量文本数据中提取出结构化的信息。
信息抽取涉及到实体识别、关系抽取等技术，通过分析文本中的实体和实体之间的关系，从而获取有用的信息。

六、机器翻译
机器翻译是自然语言处理的一个经典问题，其目标是将一种语言的文本翻译成另一种语言的文本。
机器翻译涉及到词汇翻译、句法结构的转换等复杂任务。统计机器翻译和神经网络机器翻译是两种常见的机器翻译方法。

七、情感分析
情感分析是自然语言处理中的一个应用领域，其目标是分析文本中的情感倾向。
通过情感分析，计算机可以了解人们对于特定事物或事件的情感态度。
情感分析的基本原理是通过文本中的词语、句子结构等信息来推测作者的情感倾向。

八、对话系统
对话系统是自然语言处理中的一个前沿领域，其目标是使计算机能够理解和生成自然语言对话。
对话系统涉及到自然语言生成、对话管理等复杂任务。
基于规则的对话系统和基于深度学习的对话系统是两种常见的对话系统设计方法。


***** LORA微调
LoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。
LoRA的核心思想是，在冻结预训练模型权重后，将可训练的低秩分解矩阵注入到的Transformer架构的每一层中，从而大大减少了在下游任务上的可训练参数量。
在推理时，对于使用LoRA的模型来说，可直接将原预训练模型权重与训练好的LoRA权重合并，因此在推理时不存在额外开销。
1. 旁路降维与升维操作。
2. 固定预训练模型参数。
3. 参数初始化
4. 微调过程

目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，
2021年微软提出的 LORA，
斯坦福提出的 Prefix-Tuning，
谷歌提出的 Prompt Tuning，
2022年清华提出的 P-tuning v2。

这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：

Adapter Tuning 增加了模型层数，引入了额外的推理延迟
Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能
P-tuning v2 很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差

基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：

模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。
假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出低秩自适应（LoRA）方法。

LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。






***** Transformer的一些重要组成部分和特点：
原文链接：https://blog.csdn.net/weixin_42475060/article/details/121101749
1、自注意力机制（Self-Attention）：
	这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。
2、多头注意力（Multi-Head Attention）：
	Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。
3、堆叠层（Stacked Layers）：
	Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。
4、位置编码（Positional Encoding）：
	由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。
5、残差连接和层归一化（Residual Connections and Layer Normalization）：
	这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。
6、编码器和解码器：
	Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。


Transformer的基本组成与原理
Transformer 模型主要由两个部分组成：
编码器（Encoder） 和 解码器（Decoder），这两个部分都采用了自注意力机制和前馈神经网络（Feed-Forward Neural Networks）。
整个架构可以用来处理序列到序列的任务（如机器翻译），也可以简化为仅使用编码器或解码器的任务（如文本分类、语言模型等）。

***** transformer的优缺点：
优点：
1、效果好
2、可以并行训练，速度快
3、很好的解决了长距离依赖的问题
缺点：
完全基于self-attention，对于词语位置之间的信息有一定的丢失，虽然加入了positional encoding来解决这个问题，但也还存在着可以优化的地方。
                    
原文链接：https://blog.csdn.net/weixin_42475060/article/details/121101749








https://www.bilibili.com/opus/932044096958103591

Open-Sora
https://github.com/hpcaitech/Open-Sora/blob/main/docs/zh_CN/README.md

SVD开源！
https://github.com/Stability-AI/generative-models


https://github.com/harry0703/MoneyPrinterTurbo
2024-04-16 v1.1.2 新增了9种Azure的语音合成声音，需要配置API KEY，该声音合成的更加真实。


SDXL Turbo官方介绍：https://stability.ai/news/stability-ai-sdxl-turbo
SDXL Turbo的HuggingFace开源地址：https://huggingface.co/stabilityai/sdxl-turbo



https://github.com/zhayujie/chatgpt-on-wechat
