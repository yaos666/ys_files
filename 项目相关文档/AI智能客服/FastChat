模型默认存放路径：
/root/.cache/modelscope/hub/qwen/Qwen2-7B-Instruct

# 设置pip全局镜像 (加速下载)
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/simple/



一、自定义数据训练：
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model_type qwen-7b-chat \
    --model_id_or_path weights/qwen/Qwen-7B-Chat \
    --dataset dataset/Processed_Dabby.jsonl \
    --output_dir output


CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model_type qwen-7b-chat \
    --model_id_or_path weights/qwen/Qwen-7B-Chat \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --num_train_epochs 30 \
    --eval_steps 1 \
    --logging_steps 1 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 'XLAI' \
    --model_author 姚随 xlys

CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model_type qwen-7b-chat \
    --model_id_or_path weights/qwen/Qwen-14B-Chat \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --num_train_epochs 10 \
    --eval_steps 1 \
    --logging_steps 1 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 'XLAI' \
    --model_author 姚随 星链ys



CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model_type qwen-7b-chat \
    --model_id_or_path /root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --num_train_epochs 3 \
    --eval_steps 1 \
    --logging_steps 1 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 'XLAI' \
    --model_author 姚随 ys



CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 \
NPROC_PER_NODE=6 \
swift sft \
    --model_type qwen2-57b-a14b-instruct \
    --model_id_or_path /root/autodl-tmp/yaosui/qwen2/Qwen2-57B-A14B-Instruct \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl self-cognition#500 alpaca-zh#500 alpaca-en#500 \
    --num_train_epochs 10 \
    --eval_steps 10 \
    --logging_steps 10 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 'Star Chain Intelligence' \
    --model_author 深圳市星链物联网科技有限公司 'Shenzhen Xinglian Internet of Things Intelligent Technology Co.,Ltd.'

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 \
swift sft \
    --model_type qwen-14b-chat \
    --model_id_or_path qwen/qwen-14b-chat \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --num_train_epochs 10 \
    --eval_steps 1 \
    --logging_steps 1 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 'Star Chain Intelligence' \
    --model_author 星链物联网智能科技有限公司 'Xinglian IoT Intelligent Technology Co Ltd'



CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 \
NPROC_PER_NODE=2 \
swift sft \
    --model_type qwen-14b-chat \
    --model_id_or_path /root/autodl-tmp/yaosui/qwen-14b-chat \
    --custom_train_dataset_path dataset/1_Shuffled_Processed_Dabby.jsonl self-cognition#1000 alpaca-zh#1000 alpaca-en#1000 \
    --num_train_epochs 5 \
    --eval_steps 1 \
    --logging_steps 1 \
    --metric_warmup_step 5 \
    --warmup_steps 5 \
    --gpu_memory_fraction 0.99 \
    --batch_size 8 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --sft_type lora \
    --tuner_backend peft \
    --dtype AUTO \
    --output_dir output \
    --max_length 2048 \
    --use_loss_scale true \
    --gradient_accumulation_steps 16 \
    --learning_rate 1e-4 \
    --use_flash_attn true \
    --eval_steps 1 \
    --save_steps 50 \
    --train_dataset_sample 5000 \
    --check_dataset_strategy warning \
    --gradient_checkpointing false \
    --weight_decay 0.1 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --save_total_limit 2 \
    --lora_target_modules ALL \
    --lora_rank 8 \
    --lora_alpha 32 \
    --self_cognition_sample 10 \
    --model_name 星链智能 'Star Chain Intelligence' \
    --model_author 星链物联网智能科技有限公司 'Xinglian IoT Intelligent Technology Co Ltd'
    


CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 \
nproc_per_node=2 \
swift sft \
    --model_type qwen-14b-chat \
    --model_id_or_path qwen/qwen-14b-chat \
    --model_revision master \
    --sft_type lora \
    --tuner_backend swift \
    --dtype AUTO \
    --output_dir output \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl ms-agent-for-agentfabric-default#1000 ms-agent-for-agentfabric-addition#1000 \
    --train_dataset_mix_ratio 2.0 \
    --train_dataset_sample -1 \
    --num_train_epochs 10 \
    --max_length 2048 \
    --check_dataset_strategy warning \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0.05 \
    --lora_target_modules ALL \
    --self_cognition_sample 3000 \
    --model_name 星链智能 'Star Chain Intelligence' \
    --model_author 星链物联网智能科技有限公司 'Xinglian IoT Intelligent Technology Co Ltd' \
    --gradient_checkpointing true \
    --batch_size 2 \
    --weight_decay 0.1 \
    --learning_rate 5e-5 \
    --gradient_accumulation_steps 4 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10







 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 python llm_sft.py   --model_type qwen2-57b-a14b-instruct    --model_id_or_path /root/autodl-tmp/yaosui/qwen2/Qwen2-57B-A14B-Instruct   --custom_train_dataset_path /root/autodl-tmp/yaosui/dataset/1_Shuffled_Processed_Dabby.jsonl self-cognition#1000 alpaca-zh#1000 alpaca-en#1000  --batch_size 4   --sft_type lora   --tuner_backend peft   --dtype AUTO   --output_dir output   --max_length 2048   --use_loss_scale true   --gradient_accumulation_steps 16   --learning_rate 1e-4   --use_flash_attn true   --eval_steps 1   --save_steps 50   --train_dataset_sample 5000   --num_train_epochs 10   --check_dataset_strategy warning    --gradient_checkpointing false   --weight_decay 0.1   --max_grad_norm 0.5   --warmup_ratio 0.03   --save_total_limit 2   --logging_steps 1   --use_flash_attn false    --lora_target_modules ALL   --lora_rank 8   --lora_alpha 32  --model_name 星链智能 'Star Chain Intelligence'   --model_author 星链物联网智能科技有限公司 'Xinglian IoT Intelligent Technology Co Ltd'



 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 NPROC_PER_NODE=6 python llm_sft.py   --model_type qwen2-57b-a14b-instruct    --model_id_or_path /root/autodl-tmp/yaosui/qwen2/Qwen2-57B-A14B-Instruct   --custom_train_dataset_path /root/autodl-tmp/yaosui/dataset/1_Shuffled_Processed_Dabby.jsonl self-cognition#1000 alpaca-zh#1000 alpaca-en#1000  --batch_size 4   --sft_type lora   --tuner_backend peft   --dtype AUTO   --output_dir output   --max_length 2048   --use_loss_scale true   --gradient_accumulation_steps 16   --learning_rate 1e-4   --use_flash_attn true   --eval_steps 1   --save_steps 50   --train_dataset_sample 5000   --num_train_epochs 10   --check_dataset_strategy warning    --gradient_checkpointing false   --weight_decay 0.1   --max_grad_norm 0.5   --warmup_ratio 0.03   --save_total_limit 2   --logging_steps 1   --use_flash_attn false    --lora_target_modules ALL   --lora_rank 8   --lora_alpha 32  --model_name 星链智能 'Star Chain Intelligence'   --model_author 星链物联网智能科技有限公司 'Xinglian IoT Intelligent Technology Co Ltd'

 

# Experimental environment: 2 * 3090, 2 * V100, ...
# 2 * 24GB GPU memory
CUDA_VISIBLE_DEVICES=0,1 \
NPROC_PER_NODE=2 \
swift sft \
    --model_type qwen-7b-chat \
    --dataset alpaca-zh alpaca-en \
    --train_dataset_sample 500 \
    --eval_steps 20 \
    --logging_steps 5 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 500 \
    --model_name 小黄 'Xiao Huang' \
    --model_author 魔搭 ModelScope \



CUDA_VISIBLE_DEVICES=0,1,2,3 \
swift sft \
    --model_type qwen-14b-chat \
    --model_id_or_path qwen/Qwen-14B-Chat \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --num_train_epochs 10 \
    --eval_steps 1 \
    --logging_steps 1 \
    --output_dir output \
    --lora_target_modules ALL \
    --self_cognition_sample 10 \
    --model_name 星链智能 XLAI \
    --model_author 星链物联网智能科技有限公司 XL






    










二、模型推理：
CUDA_VISIBLE_DEVICES=0 swift infer \
--model_type qwen-7b-chat \
--model_id_or_path \
/root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat \
--ckpt_dir \
/root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v0-20240713-095226/checkpoint-16


CUDA_VISIBLE_DEVICES=0 swift infer \
--model_type qwen-7b-chat
--model_id_or_path \
/root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat \
--ckpt_dir \
/root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v9-20240716-143335/checkpoint-365




查看训练过程：
tensorboard --logdir=/root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v3-20240713-122740/runs



CUDA_VISIBLE_DEVICES=0 swift infer \
--model_id_or_path \
/root/autodl-tmp/yaosui/swift/weights/qwen/Qwen-7B-Chat \
--ckpt_dir \
/root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v7-20240716-135402/checkpoint-462





三、Merge LoRA + 量化模型
*** 环境准备：https://modelscope.cn/docs/LLM%E9%87%8F%E5%8C%96%E6%96%87%E6%A1%A3
############################################################################################
# 设置pip全局镜像 (加速下载)
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
# 安装ms-swift
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[llm]'

# 使用awq量化:
# autoawq和cuda版本有对应关系，请按照`https://github.com/casper-hansen/AutoAWQ`选择版本
pip install autoawq -U

# 使用gptq量化:
# auto_gptq和cuda版本有对应关系，请按照`https://github.com/PanQiWei/AutoGPTQ#quick-installation`选择版本
pip install auto_gptq -U

# 使用bnb量化：
pip install bitsandbytes -U

# 使用hqq量化：
# 需要transformers版本>4.40，从源码安装
pip install git+https://github.com/huggingface/transformers
pip install hqq
# 如果要兼容训练，需要从源码安装peft
pip install git+https://github.com/huggingface/peft.git

# 使用eetq量化：
# 需要transformers版本>4.40，从源码安装
pip install git+https://github.com/huggingface/transformers
# 参考https://github.com/NetEase-FuXi/EETQ
git clone https://github.com/NetEase-FuXi/EETQ.git
cd EETQ/
git submodule update --init --recursive
pip install .
# 如果要兼容训练，需要从源码安装peft
pip install git+https://github.com/huggingface/peft.git

# 环境对齐 (通常不需要运行. 如果你运行错误, 可以跑下面的代码, 仓库使用最新环境测试)
pip install -r requirements/framework.txt  -U
pip install -r requirements/llm.txt  -U
############################################################################################


1、merge lora 并且进行量化：
# awq
CUDA_VISIBLE_DEVICES=0 swift export \
    --model_type qwen-7b-chat \
    --quant_bits 4 \
    --dataset dataset/Processed_Dabby.jsonl \
    --merge_lora true \
    --quant_method awq \
    --model_id_or_path /root/autodl-tmp/yaosui/swift/weights/qwen/Qwen-7B-Chat \
    --ckpt_dir /root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v3-20240713-122740/checkpoint-120

CUDA_VISIBLE_DEVICES=0 swift export \
    --model_type qwen-7b-chat \
    --quant_bits 4 \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --merge_lora true \
    --quant_method awq \
    --model_id_or_path /root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat \
    --ckpt_dir /root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v10-20240723-170355/checkpoint-182


# gptq
CUDA_VISIBLE_DEVICES=0 swift export \
    --model_type qwen-7b-chat \
    --quant_bits 4 \
    --dataset dataset/Processed_Dabby.jsonl \
    --merge_lora true \
    --quant_method gptq \
    --model_id_or_path /root/autodl-tmp/yaosui/swift/weights/qwen/Qwen-7B-Chat \
    --ckpt_dir /root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v3-20240713-122740/checkpoint-120

CUDA_VISIBLE_DEVICES=0 swift export \
    --model_type qwen-14b-chat \
    --quant_bits 4 \
    --dataset dataset/1_Shuffled_Processed_Dabby.jsonl \
    --merge_lora true \
    --quant_method gptq \
    --model_id_or_path /root/autodl-tmp/yaosui/weights/qwen/Qwen-14B-Chat \
    --ckpt_dir /root/autodl-tmp/yaosui/swift/output/qwen-14b-chat/v2-20240723-162253/checkpoint-188

2、量化后的模型推理
# awq
CUDA_VISIBLE_DEVICES=0 swift infer --ckpt_dir output/qwen-7b-chat/v3-20240713-122740/checkpoint-120-awq-int4

推理时间非常慢！！！





四、VLLM推理加速
CUDA_VISIBLE_DEVICES=0 swift infer \
    --ckpt_dir output/qwen-7b-chat/v3-20240713-122740/checkpoint-120-awq-int4 \
    --infer_backend vllm



五、web端界面尝试：
git clone https://github.com/lm-sys/FastChat.git
cd FastChat
pip3 install "fschat[model_worker,webui]"

1、 启动controller服务：
python3 -m fastchat.serve.controller --host 127.0.0.1 --port 21001

2、 启动work服务：
FASTCHAT_USE_MODELSCOPE=true python3 -m fastchat.serve.model_worker --model-path /root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat --revision v1.0.0 --host 127.0.0.1 --port 21002

python3 -m fastchat.serve.vllm_worker --model-path /root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v3-20240713-122740/checkpoint-120-awq-int4 --revision v1.0.0 --host 127.0.0.1 --port 21002

3、启动Gradio web服务，运行界面进行推理:
python3 -m fastchat.serve.gradio_web_server




python3 -m fastchat.serve.controller --host 127.0.0.1 --port 21001
FASTCHAT_USE_MODELSCOPE=true python3 -m fastchat.serve.model_worker --model-path /root/autodl-tmp/yaosui/weights/qwen/Qwen-7B-Chat --revision v1.0.0 --host 127.0.0.1 --port 21002
python3 -m fastchat.serve.gradio_web_server





1、openai接口方式启动服务：
python -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen-7b-chat \
    --model /root/autodl-tmp/yaosui/swift/output/qwen-7b-chat/v10-20240723-170355/checkpoint-182-gptq-int4 \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.3 \
    --tensor-parallel-size 1 \
    --trust-remote-code




python -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen-14b-chat \
    --model /root/autodl-tmp/yaosui/weights/qwen/Qwen-14B-Chat \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.9 \
    --tensor-parallel-size 1 \
    --trust-remote-code


python -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen2-7b-chat-instruct \
    --model /root/autodl-tmp/yaosui/weights/qwen/Qwen2-7B-Instruct \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.9 \
    --tensor-parallel-size 1 \
    --trust-remote-code



python3 -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen1half-7b-chat \
    --model /home/weight/qwen/Qwen1___5-7B-Chat-GPTQ-Int4 \
    --max-model-len 2048 \
    --gpu-memory-utilization 1 \
    --tensor-parallel-size 1 \
    --trust-remote-code
    

客户端测试：

curl http://localhost:8000/v1/models

curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "qwen-14b-chat",
        "prompt": "你好",
        "max_tokens": 50,
        "temperature": 0
    }'


curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "qwen2-7b-chat-instruct",
        "prompt": "你好",
        "max_tokens": 50,
        "temperature": 0,
        "stream": true
    }'


curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "qwen2-7b-chat-instruct",
        "prompt": "你好,你是谁",
        "max_tokens": 50,
        "temperature": 0,
        "stream": false
    }'



curl http://172.29.113.125:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "qwen1half-7b-chat",
        "prompt": "你好,你是谁",
        "max_tokens": 50,
        "temperature": 0,
        "stream": false
    }'





六、连接服务web
方式一：
streamlit run web_demo_stream.py
streamlit run /root/autodl-tmp/chatBot.py --server.address 127.0.0.1 --server.port 6006
streamlit run web_demo.py --server.port 6006



方式二：
1、模型启动

注意：添加max_num_batched_tokens实现多轮对话限制，需要max-model-len同步
python3 -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen1half-7b-chat \
    --model /home/weight/qwen/Qwen1___5-7B-Chat-GPTQ-Int4 \
    --max-model-len 1024 \
    --max_num_batched_tokens=1024 \
    --max_num_seqs=256 \
    --gpu-memory-utilization 1 \
    --tensor-parallel-size 1 \
    --trust-remote-code


RAY_memory_monitor_refresh_ms=0 CUDA_VISIBLE_DEVICES=0
python -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen1half-4b-chat \
    --model  /root/autodl-tmp/Qwen1.5-4B-Chat-GPTQ-Int4 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.9 \
    --tensor-parallel-size 1 \
    --enforce-eager 


python3 -m vllm.entrypoints.openai.api_server \
    --served-model-name qwen1half-0.5b-chat \
    --model /home/models/qwen1.5-0.5b-chat-gptq-int4 \
    --max-model-len 1024 \
    --gpu-memory-utilization 0.95 \
    --tensor_parallel_size 1 \
    --trust-remote-code \
    --dtype half \
    --quantization gptq



2、将端口映射到公网(cmd终端内)
ssh -R 80:127.0.0.1:8000 serveo.net


3、如果进入docker容器里面启动模型的：
注意：如果集成在一起，则不需要
需要将docker内外网络打通：(ubuntu终端内)
docker run -it --gpus all --name vllm_openai -v /home/yaosui:/home -p 8000:8000 registry.cn-hangzhou.aliyuncs.com/xinshang_14/llm_7b:1.0.1 /bin/bash
docker network connect vllm_openai




4、数据库、客户端界面显示
参考fastgpt界面，接入onapi：
https://juejin.cn/post/7344628375380918310
https://doc.fastgpt.in/docs/development/docker/



5、https://doc.fastgpt.in/docs/development/docker/
根据github下载到/home/yaosui/fastgpt
config.json  docker-compose.yml 

## 修改config.json文件中的模型相关参数
## 修改docker-compose.yml 文件中的镜像相关参数
## 映射好公网ip和oneapi的key之后，修改docker-compose.yml相应位置

启动容器服务：
docker-compose up -d
停止容器服务：
docker-compose down


6、oneapi中的操作：127.0.0.1：3001
## 渠道中编辑：
类型：自定义渠道
容器内启动
Base URL：http://vllm_openai:8000
容器外集成需要外网映射ip+端口
分组：default
密钥：在令牌中生成
最后测试是否成功


7、FastGPT中进行界面配置：
127.0.0.1：3000